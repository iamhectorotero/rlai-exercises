{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1 \n",
    "  \n",
    "## Question:\n",
    "Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions,\n",
    "and rewards. Make the three examples as _different_ from each other as possible. The framework is abstract and\n",
    "flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.\n",
    "\n",
    "## Answer:\n",
    "Example 1: Hairdresser agent\n",
    "\n",
    "- States: the state of the hair and the desired cut of the client. The state of the hair and desired cut could be\n",
    "encoded as arrays of the length of the hair in a set of predetermined areas the head is divided in.\n",
    "- Actions: using the scissors or the clipper (and what accessory) and the area to be cut.\n",
    "- Rewards: negative rewards for the client complaints or imprecisions in the cut and positive rewards for tips.\n",
    "\n",
    "Example 2: DJ agent\n",
    "\n",
    "- States: a measure of how much people are dancing and singing to the song being played and the song currently playing.\n",
    "- Actions: given a setlist of 5000 songs, selecting the next song to be played (or a combination of them) and a type of\n",
    "transition between the songs.\n",
    "- Rewards: negative rewards given by people leaving the club faster than expected, positive rewards given by the level\n",
    "of danciness/_singiness_ in the club.\n",
    "\n",
    "Example 3: Texas Hold'em Poker player agent\n",
    "\n",
    "- States: The two cards in its hand and the cards showing in the table.\n",
    "- Actions: Check, call, raise, or fold.\n",
    "- Rewards: The money obtained or lost after playing one hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 \n",
    "  \n",
    "## Question:\n",
    "Is the MDP framework adequate to usefully represent *all* goal-directed learning tasks?\n",
    "Can you think of any clear exceptions?\n",
    "\n",
    "## Answer:\n",
    "We can try to think about it in terms of the limitations a finite MDP imposes in a problem definition and whether any\n",
    "approximations could exist within the framework.\n",
    "\n",
    "1) The Markov property, if not only the previous state and action selected influence which will be the next state. An\n",
    "example of this could be a modified game of chess where the order in which the pieces were moved affects the possible\n",
    "moves. This could be encoded within the MDP, an state would be composed of the position of the pieces and when\n",
    "they have been moved (skyrocketing the amount of available states making it a more difficult learning environment).\n",
    "If the sequence information wasn't available (or only partially available) at the time of making a decision,\n",
    "information from the past that can affect the next state would be missing, breaking the Markov property.\n",
    "\n",
    "2) The action and state sets must be finite. Any problem with infinite available actions or states would need\n",
    "alternative representations, such as grouping them into subsets and use these sets. An example of this could be a\n",
    "problem where the states are the natural numbers and we have to define intervals (e.g. negative numbers, numbers in the\n",
    "range 0-25, 25-200, and 200-∞) or where the actions can be a string of any size and we restrict it to a discrete number\n",
    "of lengths (e.g. only generate strings with length 3, 5, 8, 13, 21 and 34).\n",
    "\n",
    "3) Rewards must be numerical. If the rewards the environment gives back are not numerical we would need to encode them\n",
    "as numbers. This can be a highly difficult task as the rewards may not translate naturally to numbers. For example,\n",
    "if the rewards were your family's verbal feedback on how good the meal you prepared was, it would be difficult to\n",
    "convert it into a number and capture correctly its intensity (e.g. What's a better feedback? \"It was really good\" or\n",
    "\"I have enjoyed the meal a lot\"). If we opt for a simpler reward encoding, distinguishing only negative, neutral and\n",
    "positive comments, this may not capture perfectly the information given.\n",
    "\n",
    "The first example is, as far as I understand, a clear exception of the MDP-framework not being an appropriate\n",
    "representation. Apart from this, some of the points mentioned (or their combination) may difficult largely the task for\n",
    "an agent, making it really hard to learn anything valuable from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3\n",
    "  \n",
    "## Question:\n",
    "Consider the problem of driving. You could define the actions in\n",
    "terms of the accelerator, steering wheel, and brake, that is, where your body meets\n",
    "the machine. Or you could define them farther out - say, where the rubber meets the\n",
    "road, considering your actions to be tire torques. Or you could define them farther\n",
    "in - say, where your brain meets your body, the actions being muscle twitches to\n",
    "control your limbs. Or you could go to a really high level and say that your actions\n",
    "are your choices of where to drive. What is the right level, the right place to draw\n",
    "the line between agent and environment? On what basis is one location of the line\n",
    "to be preferred over another? Is there any fundamental reason for preferring one\n",
    "location over another, or is it a free choice?\n",
    "\n",
    "## Answer:\n",
    "The limit of the actions should be at the functional point, being the \n",
    "point at which is the agent makes the decision to take an action that\n",
    "the action always occurs in the same way every time. Above that point\n",
    "it is better to think of the agent as having sub-goals and goals, where\n",
    "something like walking to the door is a goal with the sub-goals of \n",
    "moving the legs, with the action of applying hydraulic pressure to\n",
    "particular points.\n",
    "\n",
    "However this does depend on the reliability of the system, and what\n",
    "reliability you require. If 99% of sub-goals are successful then it may\n",
    "be easy to convert them into actions. Abstracting in this way makes\n",
    "reaching larger goals easier as it dramatically reduces the\n",
    "action-space needed to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.4\n",
    "\n",
    "# Question:\n",
    "Give a table analogous to that in Example 3.3, but for p(s',r|s, a). It should have columns for s, a, s', r, and p(s', r|s, a), and a row for every 4-tuple for which p(s',r|s, a) > 0.\n",
    "\n",
    "# Answer\n",
    "Since there is a single reward defined for each triplet (s, a, s'), the table is the same filtering the lines with p(s'|s,a)=0.\n",
    "\n",
    "This fulfills the formula (3.4):\n",
    "\\begin{equation*}\n",
    "p(s'|s, a) = \\sum_{r \\in R} p(s',r|s, a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.5\n",
    "\n",
    "### Question:\n",
    "The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to\n",
    "apply to episodic tasks. Show that you know the modifications needed by giving the modified version\n",
    "of (3.3)\n",
    "\n",
    "### Answer:\n",
    "The original formula is:\n",
    "    \n",
    "\\begin{equation*}\n",
    "        \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a) = 1, \\forall s \\in S, a \\in A(s)\n",
    "\\end{equation*}\n",
    "\n",
    "according to the definitions in 3.3, for episodic tasks the set of terminal and non-terminal states can be denoted as S+. Therefore, the formula changes to:\n",
    "\\begin{equation*}\n",
    "        \\sum_{s' \\in S} \\sum_{r \\in R} p(s', r|s, a) = 1, \\forall s \\in S^+, a \\in A(s)\n",
    "\\end{equation*}\n",
    "as the dynamics of the MDP in an episodic task include as a possible transition those ending in a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.6\n",
    "  \n",
    "### Question:\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for -1 upon failure. What then would the\n",
    "return be at each time? How does this return differ from that in the discounted,\n",
    "continuing formulation of this task?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "The formula would change to:\n",
    "\n",
    "\\begin{equation*}\n",
    "G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}\n",
    "\\end{equation*}\n",
    "\n",
    "In the limit (very large T), both returns would be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.7\n",
    "  \n",
    "## Question:\n",
    "Imagine that you are designing a robot to run a maze. You decide\n",
    "to give it a reward of +1 for escaping from the maze and a reward of zero at all\n",
    "other times. The task seems to break down naturally into episodes - the successive\n",
    "runs through the maze - so you decide to treat it as an episodic task, where the goal\n",
    "is to maximize expected total reward (3.1). After running the learning agent for a\n",
    "while, you find that it is showing no improvement in escaping from the maze. What\n",
    "is going wrong? Have you effectively communicated to the agent what you want it\n",
    "to achieve?\n",
    "\n",
    "## Answer:\n",
    "This is likely an exploration issue where the agent is unable to find\n",
    "the exit the first time and therefore doesn't know there's anything\n",
    "better than 0 as a reward. Potential solutions include having each\n",
    "non-goal state be worth -1, and/or extending the episode length. This\n",
    "would mean states the agents visits a lot (particularly around the start)\n",
    "will get worse and worse values so it will want to move away from there\n",
    "and eventually find the goal (essentially reaching the goal stops it\n",
    "being in pain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.8\n",
    "\n",
    "# Answer\n",
    "Suppose gamma = 0.5 and the following sequence of rewards is received R_1 = -1, R_2 = 2, R_3 = 6, R_4 = 3 and R_5 = 3, with T = 5. What are G_0, G_1, ..., G_5? Hint: Work backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_0 = 2.0\n",
      "G_1 = 6.0\n",
      "G_2 = 8.0\n",
      "G_3 = 4.0\n",
      "G_4 = 2.0\n"
     ]
    }
   ],
   "source": [
    "r = [-1, 2, 6, 3, 2]\n",
    "gamma = 0.5\n",
    "\n",
    "for g_i in range(len(r)):\n",
    "    ret = sum([gamma**i * r_i for i, r_i in enumerate(r[g_i:])])    \n",
    "    print(\"G_\"+ str(g_i) + \" = \" + str(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.9\n",
    "\n",
    "## Question\n",
    "Suppose gamma=0.9 and the reward sequence is R_1 = 2 followed by an infinite sequence of 7s. What are G_1 and G_0?\n",
    "\n",
    "## Answer\n",
    "\\begin{equation*}\n",
    "G_0 = 2 + \\gamma\\ 7 + \\gamma^2\\ 7 + ... = 2 + \\gamma\\ ( \\sum_{k=0}^{\\infty} \\gamma^k\\ 7 ) = 2 + \\gamma\\ 7\\ ( \\sum_{k=0}^{\\infty} \\gamma^k) =  2 + \\gamma\\ 7\\ (\\frac{1}{1 - \\gamma}) = 2 + 0.9 \\times 7 \\times 10 = 65\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "G_1 = 7 + \\gamma\\ 7 + \\gamma^2\\ 7 + ... = 7\\ ( \\sum_{k=0}^{\\infty} \\gamma^k) = 7 \\frac{1}{1 - \\gamma} = 70\n",
    "\\end{equation*}\n",
    "\n",
    "This results fulfills:\n",
    "\n",
    "\\begin{equation*}\n",
    "G_0 = R_1 + \\gamma G_1 = 2 + 0.9 \\times 70 = 65\n",
    "\\end{equation*}\n",
    "\n",
    "This problem can also be solved through the equations below (note that G_2 = G_1 = G_n where n > 0):\n",
    "\n",
    "\\begin{equation*}\n",
    "G_0 = R_1 + 0.9\\ G_1\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "G_1 = R_2 + 0.9\\ G_1\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.10\n",
    "\n",
    "## Question\n",
    "Prove (3.10)\n",
    "\n",
    "## Answer\n",
    "\\begin{equation*}\n",
    "G_t = \\sum_{k=0}^\\infty y_k = lim_{n \\rightarrow \\infty} (1 + \\gamma + \\gamma^2 + ... + \\gamma^n) = lim_{n \\rightarrow \\infty} \\frac{(1 +   \\gamma + \\gamma^2 + ... + \\gamma^n) (1 - \\gamma)}{(1 - \\gamma)} = lim_{n \\rightarrow \\infty} \\frac{1 - \\gamma^{n+1}}{1 - \\gamma} = \\frac{1}{1 - \\gamma}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.11\n",
    "\n",
    "## Question\n",
    "If the current state is S_t, and actions are selected according to stochastic policy pi, then what is the expectation of R_t+1 in terms of pi and the four-argument function p(3.2)?\n",
    "\n",
    "## Answer\n",
    "\\begin{equation*}\n",
    "\\mathbb{E} [R_{t+1} | S_t=s, \\pi] = \\sum_a \\pi(a|s) \\sum_{s',r} r\\ p(s', r|s, a)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.12\n",
    "## Question\n",
    "Give an equation for v_pi in terms of q_pi and pi.\n",
    "\n",
    "## Answer\n",
    "\\begin{equation*}\n",
    "v_\\pi(s) = \\sum_{a\\in A(s)} q_\\pi(s,a) \\pi(a|s)\n",
    "\\end{equation*}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.13\n",
    "## Question\n",
    "Give an equation for q_pi in terms of v_pi and the four-argument p\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.14\n",
    "## Question\n",
    "The Bellman equation (3.14) must hold for each state for the value function v_pi shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4 and +0.7. (These numbers are accurate only to one decimal place.)\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.15\n",
    "## Question\n",
    "In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant c to all the rewards adds a constant, v_c, to the values of all states, and thus\n",
    "does not affect the relative values of any states under any policies. What is v_c in terms of c and gamma?\n",
    "\n",
    "\n",
    "## Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.16\n",
    "## Question\n",
    "Now consider adding a constant c to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.17\n",
    "## Question\n",
    "What is the Bellman equation for action values, that is, for q_pi? It must give the action value q_pi(s, a) in terms of the action values, q_pi(s',a'), of possible successors to the state–action pair (s, a). Hint: the backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.18\n",
    "## Question\n",
    "The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n",
    "\n",
    "Give the equation corresponding to this intuition and diagram for the value at the root node, v_pi(s), in terms of the value at the expected leaf node, q_pi(s, a), given S_t=s. This equation should include an expectation conditioned on following the policy, \\pi. Then give a second equation in which the expected value is written out explicitly in terms of \\pi(a|s) such that no expected value notation appears in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.19\n",
    "The value of an action, q_pi(s, a), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state–action pair) and branching to the possible next states:\n",
    "\n",
    "Give the equation corresponding to this intuition and diagram for the action value, q_pi(s, a), in terms of the expected next reward, Rt+1, and the expected next state value,v_pi(St+1), given that St=s and At=a. This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of p(s',r|s, a) defined by (3.2), such that no expected value notation appears in the equation.\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.20\n",
    "Draw or describe the optimal state-value function for the golf example.\n",
    "\n",
    "Exercise 3.21\n",
    "Draw or describe the contours of the optimal action-value function for\n",
    "putting, q\\*(s,putter), for the golf example.\n",
    "\n",
    "Exercise 3.22\n",
    "Consider the continuing MDP shown on to the right. The only decision to be made is that in the top state,\n",
    "where two actions are available, left and right. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, pi_left and pi_right. What policy is optimal if\n",
    "gamma=0? If gamma=0.9? If gamma=0.5?\n",
    "\n",
    "Exercise 3.23\n",
    "Give the Bellman equation for q_pi for the recycling robot.\n",
    "\n",
    "Exercise 3.24\n",
    "Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.\n",
    "\n",
    "Exercise 3.25\n",
    "Give an equation for v_* in terms of q_*.\n",
    "\n",
    "Exercise 3.26\n",
    "Give an equation for q_* in terms of v* and the four-argument p.\n",
    "\n",
    "Exercise 3.27\n",
    "Give an equation for \\pi_* in terms of q*.\n",
    "\n",
    "Exercise 3.28\n",
    "Give an equation for pi* in terms of v* and the four-argument p.\n",
    "\n",
    "Exercise 3.29\n",
    "Rewrite the four Bellman equations for the four value functions (v_pi,v*,q_pi,and q*) in terms of the three argument function p (3.4) and the two-argument function r (3.5)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
